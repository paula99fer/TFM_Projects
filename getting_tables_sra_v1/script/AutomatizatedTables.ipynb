{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d01c937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from statistics import mean\n",
    "import time \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c630345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input path\n",
    "path_tablajose = 'C:/Users/paula/OneDrive/Documentos/0. Master Biologia Computacional UPM/TFM_CNB/Automatizated process/Tables/DatabaseSamplesJOSE.csv'\n",
    "path_tablasra = 'C:/Users/paula/OneDrive/Documentos/0. Master Biologia Computacional UPM/TFM_CNB/Automatizated process/Tables/SraRunInfo.csv'\n",
    "\n",
    "path_tablaweb = 'C:/Users/paula/OneDrive/Documentos/0. Master Biologia Computacional UPM/TFM_CNB/Automatizated process/Tables/SRAinformation.tsv'\n",
    "path_infotable = 'C:/Users/paula/OneDrive/Documentos/0. Master Biologia Computacional UPM/TFM_CNB/Automatizated process/Tables/sra_result.csv'\n",
    "\n",
    "#Output path\n",
    "path_output_tables = 'C:/Users/paula/OneDrive/Documentos/0. Master Biologia Computacional UPM/TFM_CNB/Automatizated process/Tables/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37076064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_3060\\2166315934.py:3: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfsra = pd.read_csv(path_tablasra)\n"
     ]
    }
   ],
   "source": [
    "#Upload files\n",
    "dfjose = pd.read_csv(path_tablajose)\n",
    "dfsra = pd.read_csv(path_tablasra)\n",
    "\n",
    "dfweb = pd.read_csv(path_tablaweb,sep='\\t')\n",
    "dfinfo = pd.read_csv(path_infotable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a016136",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MERGED BY SRX\n",
    "#Intersección: los que están en ambos.\n",
    "dfmerge = pd.merge(dfjose, dfsra, \n",
    "                   on=['Experiment'], \n",
    "                   how='inner')\n",
    "\n",
    "# dfmerge.to_csv(f'{path_output_tables}Merge.csv', index=False)\n",
    "\n",
    "#Tabla de Jose sin los comunes.\n",
    "comun = list(dfmerge['Experiment'])\n",
    "indexNames = dfjose[dfjose['Experiment'].isin(comun)].index\n",
    "dfjosedrop = dfjose.drop(indexNames)\n",
    "\n",
    "# dfjosedrop.to_csv(f'{path_output_tables}Jose(-Merge).csv', index=False)\n",
    "\n",
    "#Tabla de SRA sin los comunes.\n",
    "indexNames1 = dfsra[dfsra['Experiment'].isin(comun)].index\n",
    "dfsradrop = dfsra.drop(indexNames1)\n",
    "\n",
    "# dfsradrop.to_csv(f'{path_output_tables}Sra(-Merge).csv', index=False)\n",
    "\n",
    "\n",
    "##MERGED BY GSM\n",
    "dfjosedrop.rename(columns = {'Experiment':'geo'}, inplace = True)\n",
    "dfsradrop.rename(columns = {'SampleName':'geo'}, inplace = True)\n",
    "\n",
    "dfmerge_geo = pd.merge(dfjosedrop, dfsradrop, \n",
    "                   on=['geo'], \n",
    "                   how='inner')\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##CONCACT TWO MERGED\n",
    "dfmerge_geo.rename(columns = {'geo':'SampleName_y'}, inplace = True)\n",
    "dfmerge_geo.rename(columns = {'SampleName':'SampleName_x'}, inplace = True)\n",
    "\n",
    "merged_final = pd.concat([dfmerge, dfmerge_geo])\n",
    "# merge_final.to_csv(f'{path_output_tables}Merge_final.csv', index=False)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##SRA without the merged_final runs\n",
    "comunruns = list(merge_final['Run'])\n",
    "indexNames2 = dfsra[dfsra['Run'].isin(comunruns)].index\n",
    "dfsra_without = dfsra.drop(indexNames2)\n",
    "\n",
    "# dfsra_without.to_csv(f'{path_output_tables}Sra(-Merge_final).csv', index=False)\n",
    "\n",
    "##SRA(-Merge_final) Filter by columns values:\n",
    "dfsra_without = dfsra_without.drop(dfsra_without[(dfsra_without.LibrarySelection == 'MNase')|(dfsra_without.LibrarySelection == 'Restriction Digest')|(dfsra_without.LibrarySelection == 'padlock probes capture metho')].index)\n",
    "dfsra_without = dfsra_without.drop(dfsra_without[(dfsra_without.LibrarySource == 'GENOMIC')|(dfsra_without.LibrarySource == 'METAGENOMIC')|(dfsra_without.LibrarySource == 'TRANSCRIPTOMIC SINGLE CELL')|(dfsra_without.LibrarySource == 'METATRANSCRIPTOMIC')|(dfsra_without.LibrarySource == 'SYNTHETIC')|(dfsra_without.LibrarySource == 'VIRAL RNA')].index)\n",
    "dfsra_without = dfsra_without.drop(dfsra_without[(dfsra_without.Platform == 'LS454')|(dfsra_without.Platform == 'OXFORD_NANOPORE')|(dfsra_without.Platform == 'HELICOS')|(dfsra_without.Platform == 'PACBIO_SMRT')].index)\n",
    "dfsra_without = dfsra_without.drop(dfsra_without[(dfsra_without.Model == 'PacBio RS')|(dfsra_without.Model == 'Helicos HeliScope')|(dfsra_without.Model == 'Illumina HiSeq X')|(dfsra_without.Model == 'HiSeq X Five')|(dfsra_without.Model == 'Illumina iSeq 100')|(dfsra_without.Model == 'NextSeq 2000')|(dfsra_without.Model == 'DNBSEQ-T7')|(dfsra_without.Model == 'GridION')|(dfsra_without.Model == 'PromethION')|(dfsra_without.Model == 'Sequel II')|(dfsra_without.Model == 'NextSeq 1000')|(dfsra_without.Model == 'DNBSEQ-G400')|(dfsra_without.Model == '454 GS FLX+')|(dfsra_without.Model == 'MGISEQ-2000RS')|(dfsra_without.Model == 'PacBio RS II')|(dfsra_without.Model == 'Sequel')|(dfsra_without.Model == '454 GS')|(dfsra_without.Model == '454 GS FLX Titanium')|(dfsra_without.Model == 'MinION')].index)\n",
    "\n",
    "# dfsra_without.to_csv(f'{path_output_tables}Sra(-Merge_final)Filtered.csv', index=False)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##Rename Jose columns in merged_final and cleaning\n",
    "\n",
    "merged_final.rename(columns = {'Unnamed: 11':'Jose_1'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 12':'Jose_2'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 13':'Jose_3'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 14':'Jose_4'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 15':'Jose_5'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 16':'Jose_6'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 17':'Jose_7'}, inplace = True)\n",
    "merged_final.rename(columns = {'Unnamed: 18':'Jose_8'}, inplace = True)\n",
    "\n",
    "merged_final.Jose_1 = merged_final.Jose_1.str.strip()\n",
    "merged_final.Jose_1 = merged_final.Jose_1.str.replace('\\t', '')\n",
    "\n",
    "\n",
    "merged_final = merged_final.replace('abitotic','abiotic', regex=True)\n",
    "merged_final = merged_final.replace('Abiotic','abiotic', regex=True)\n",
    "merged_final = merged_final.replace('developemt','development', regex=True)\n",
    "merged_final = merged_final.replace('developement','development', regex=True)\n",
    "merged_final = merged_final.replace('developmet','development', regex=True)\n",
    "merged_final = merged_final.replace('homone','hormone', regex=True)\n",
    "merged_final = merged_final.replace('hormome','hormone', regex=True)\n",
    "merged_final = merged_final.replace('nioinfo','noinfo', regex=True)\n",
    "merged_final = merged_final.replace('no info','noinfo', regex=True)\n",
    "merged_final = merged_final.replace('Light','light', regex=True)\n",
    "merged_final = merged_final.replace('methylayion','methylation', regex=True)\n",
    "merged_final = merged_final.replace('nutrients','nutrient', regex=True)\n",
    "merged_final = merged_final.replace('nutirent','nutrient', regex=True)\n",
    "merged_final = merged_final.replace('chenical','chemical', regex=True)\n",
    "merged_final = merged_final.replace('chormatin','chromatin', regex=True)\n",
    "merged_final = merged_final.replace('mutatant/Ox','mutant/Ox', regex=True)\n",
    "merged_final = merged_final.replace('accessions','Accessions', regex=True)\n",
    "merged_final = merged_final.replace('Accession','Accessions', regex=True)\n",
    "merged_final = merged_final.replace('accession','Accessions', regex=True)\n",
    "merged_final = merged_final.replace('Accessionss','Accessions', regex=True)\n",
    "merged_final = merged_final.replace('accesions','Accessions', regex=True)\n",
    "\n",
    "merged_final = merged_final.replace('Accessionss','Accessions', regex=True)\n",
    "\n",
    "merged_final = merged_final.replace('methylation','chromatin', regex=True)\n",
    "merged_final = merged_final.replace('nutrient stress','nutrient', regex=True)\n",
    "merged_final = merged_final.replace('beneficial biotic','biotic', regex=True)\n",
    "merged_final = merged_final.replace('flower','development', regex=True)\n",
    "\n",
    "# print(merged_final.Jose_1.value_counts())\n",
    "\n",
    "# merged_final.to_csv(f'{path_output_tables}Merge_final_GoodClasif.csv', index=False)\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##Ordenar tabla merge final good clasification:\n",
    "\n",
    "dfweb.rename(columns={'Unnamed: 0':'BioProject','0':'Project Tittle','1':'Project Description','2':'Project Biblio'}, inplace = True)\n",
    "dfinfo.rename(columns={\"Experiment Accession\": \"Experiment\"},inplace = True)\n",
    "\n",
    "dfmerge_final_web = pd.merge(merged_final, dfweb, \n",
    "                   on=['BioProject'], \n",
    "                   how='inner')\n",
    "dfmerge_final_webinfo = pd.merge(dfmerge_final_web, dfinfo, \n",
    "                   on=['Experiment'], \n",
    "                   how='inner')\n",
    "\n",
    "dfmerge_final_webinfo.drop(['LibraryStrategy','LibrarySelection','LibrarySource'], axis=1,inplace = True)\n",
    "merge_final_ordenada = dfmerge_final_webinfo.reindex(columns=['Submission','Study Accession','Experiment','Sample Accession','Run',\n",
    "                                         'BioProject','ProjectID','BioSample','Organism Name','SampleType',\n",
    "                                         'ReleaseDate_y','LoadDate','CenterName','Submitter',\n",
    "                                         'SampleName_x','LibraryName','Library Name','SampleName_y','Library Strategy', 'Library Source','Library Selection','LibraryLayout',\n",
    "                                         'TotalReads','UniqueMappedRatio','bases','spots','spots_with_mates','avgLength','size_MB', 'Total Size, Mb', 'Total RUNs', 'Total Spots','Total Bases',\n",
    "                                         'InsertSize','InsertDev','Platform','Instrument',\n",
    "                                         'Study Title','Sample Title','Project Tittle','Project Description','Project Biblio',\n",
    "                                         'Jose_1', 'Jose_2', 'Jose_3', 'Jose_4', 'Jose_5','Jose_6', 'Jose_7', 'Jose_8',\n",
    "                                         'download_path'])\n",
    "\n",
    "merge_final_ordenada.rename(columns={\"Sample Title\": \"SampleTitle\"},inplace = True)\n",
    "merge_final_ordenada.rename(columns={\"ReleaseDate_y\": \"ReleaseDate\"},inplace = True)\n",
    "\n",
    "# merge_final_ordenada.to_csv(f'{path_output_tables}Merge_final_GoodClasif_ordenada.csv', index=False)\n",
    "\n",
    "\n",
    "##Ordenar tabla SRA(-Merge_final) good clasification:\n",
    "dfsra_without_web = pd.merge(dfsra_without, dfweb, \n",
    "                   on=['BioProject'], \n",
    "                   how='inner')\n",
    "dfsra_without_webinfo = pd.merge(dfsra_without_web, dfinfo, \n",
    "                   on=['Experiment'], \n",
    "                   how='inner')\n",
    "\n",
    "sra_without_ordenada = dfsra_without_webinfo.reindex(columns=['Submission','Study Accession','Experiment','Sample','Run',\n",
    "                                         'BioProject','ProjectID','BioSample','Organism Name','SampleType',\n",
    "                                         'ReleaseDate','LoadDate','CenterName','Submitter',\n",
    "                                         'SampleName','LibraryName','Library Name','Library Strategy', 'Library Source','Library Selection','LibraryLayout',\n",
    "                                         'TotalReads','UniqueMappedRatio','bases','spots','spots_with_mates','avgLength','size_MB', 'Total Size, Mb', 'Total RUNs', 'Total Spots','Total Bases',\n",
    "                                         'InsertSize','InsertDev','Platform','Instrument',\n",
    "                                         'Study Title','Sample Title','Project Tittle','Project Description','Project Biblio',\n",
    "                                         'Jose_1', 'Jose_2', 'Jose_3', 'Jose_4', 'Jose_5','Jose_6', 'Jose_7', 'Jose_8',\n",
    "                                         'download_path'])\n",
    "\n",
    "sra_without_ordenada.rename(columns={\"Sample Title\": \"SampleTitle\"},inplace = True)\n",
    "sra_without_ordenada.rename(columns={\"Study Title\": \"StudyTitle\"},inplace = True)\n",
    "sra_without_ordenada.rename(columns={\"Project Tittle\": \"ProjectTittle\"},inplace = True)\n",
    "\n",
    "# sra_without_ordenada.to_csv(f'{path_output_tables}Sra(-Merge_final)Filtered_ordenada.csv', index=False)\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## FILTER ABIOTIC\n",
    "\n",
    "abiotic = {'No':[],'Yes':[],'Evaluate':[]}\n",
    "countsTerms = {}\n",
    "search_words = r\"drought|submergence|salt|cold|X-ray|temperature|heat|wounding|\\sUV[\\s\\.]|radiation|metal|CO2|dry|dehydrat\\w+|flood\\w+|\\sNaCl\\s\\.|magnetic field|osmotic|water stress|salt+H2O2|H2O2|hypoxia|ozone|stress response|photorespiratory stress|oxidative|injury|carbon starvation|space\\s?flight|cell wall damage|Gamma-IR|nanoplastics|low water\"\n",
    "for row in sra_without_ordenada.itertuples():\n",
    "    pattern=','.join([str(row.StudyTitle),str(row.ProjectTittle)])\n",
    "    if re.search(search_words, pattern, re.I):\n",
    "        regular = re.search(search_words, pattern, re.I)\n",
    "        \n",
    "        g = countsTerms.get(regular.group(0), 0)\n",
    "        countsTerms[regular.group(0)] = g+1\n",
    "        \n",
    "        abiotic['Yes'].append(row.Run)\n",
    "    elif len(pattern) == 0:\n",
    "        abiotic['Evaluate'].append(row.Run)\n",
    "    else:\n",
    "        abiotic['No'].append(row.Run)\n",
    "        \n",
    "SRA_ABIOTICS = abiotic['Yes']\n",
    "dfABIOTIC = sra_without_ordenada[sra_without_ordenada['Run'].isin(SRA_ABIOTICS)]\n",
    "\n",
    "dfABIOTIC.to_excel(f'{path_output_tables}dfsra_ABIOTIC.xlsx',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14523630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Submission</th>\n",
       "      <th>Study Accession</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Run</th>\n",
       "      <th>BioProject</th>\n",
       "      <th>ProjectID</th>\n",
       "      <th>BioSample</th>\n",
       "      <th>Organism Name</th>\n",
       "      <th>SampleType</th>\n",
       "      <th>...</th>\n",
       "      <th>Project Biblio</th>\n",
       "      <th>Jose_1</th>\n",
       "      <th>Jose_2</th>\n",
       "      <th>Jose_3</th>\n",
       "      <th>Jose_4</th>\n",
       "      <th>Jose_5</th>\n",
       "      <th>Jose_6</th>\n",
       "      <th>Jose_7</th>\n",
       "      <th>Jose_8</th>\n",
       "      <th>download_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Submission, Study Accession, Experiment, Sample, Run, BioProject, ProjectID, BioSample, Organism Name, SampleType, ReleaseDate, LoadDate, CenterName, Submitter, SampleName, LibraryName, Library Name, Library Strategy, Library Source, Library Selection, LibraryLayout, TotalReads, UniqueMappedRatio, bases, spots, spots_with_mates, avgLength, size_MB, Total Size, Mb, Total RUNs, Total Spots, Total Bases, InsertSize, InsertDev, Platform, Instrument, StudyTitle, SampleTitle, ProjectTittle, Project Description, Project Biblio, Jose_1, Jose_2, Jose_3, Jose_4, Jose_5, Jose_6, Jose_7, Jose_8, download_path]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "duplicates = dfABIOTIC[dfABIOTIC.duplicated(subset=['Run'], keep=False)]\n",
    "display(duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
